{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose= mp.solutions.pose\n",
    "mp_draw= mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742448320.850271   10452 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742448320.852750   11307 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: Mesa Intel(R) UHD Graphics 620 (KBL GT2)\n",
      "I0000 00:00:1742448320.877845   10452 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742448320.880146   11318 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: Mesa Intel(R) UHD Graphics 620 (KBL GT2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1742448320.891855   11309 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742448320.924522   11310 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742448321.001837   11302 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742448321.076597   11300 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "pose=mp_pose.Pose( min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "face=mp_face_mesh.FaceMesh( max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mp_pose= mp.solutions.pose\n",
    "# mp_draw= mp.solutions.drawing_utils\n",
    "# mp_face_mesh = mp.solutions.face_mesh\n",
    "# pose=mp_pose.Pose( min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "# face=mp_face_mesh.FaceMesh( max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5)\n",
    "# cap= cv2.VideoCapture(1)\n",
    "# while cap.isOpened():\n",
    "#     # Get the current time\n",
    "#     ret, frame= cap.read()\n",
    "#     if not ret:\n",
    "#             break\n",
    "#     # Convert the frame to grayscale\n",
    "#     image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     face=pose.process(image)\n",
    "\n",
    "#     if cv2.waitKey(0) or 0xFF == ord('q'):\n",
    "#           break\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "\n",
    "# def detect_pose():\n",
    "#     mp_face_mesh = mp.solutions.face_mesh\n",
    "#     mp_pose= mp.solutions.pose\n",
    "#     face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
    "#     pose= mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    \n",
    "    \n",
    "#     mp_drawing = mp.solutions.drawing_utils\n",
    "#     mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "#     cap = cv2.VideoCapture(1)\n",
    "#     def distance(x1, x2, y1, y2):\n",
    "#         return math.sqrt((x2 - x1) ** 2 + (y2 - y1) **2)\n",
    "\n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         results = face_mesh.process(image)\n",
    "#         pose_result= pose.process(image)\n",
    "        \n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#         if results.multi_face_landmarks:\n",
    "#             for face_landmarks in results.multi_face_landmarks:\n",
    "#                 h, w, _ = image.shape\n",
    "                \n",
    "#                 # Get landmark positions\n",
    "#                 nose_tip = face_landmarks.landmark[4]  # Nose tip\n",
    "#                 nose_bridge = face_landmarks.landmark[6]  # Upper nose bridge (for V-shape reference)\n",
    "#                 left_ear = face_landmarks.landmark[234]  # Left ear landmark\n",
    "#                 right_ear = face_landmarks.landmark[454]  # Right ear landmark\n",
    "\n",
    "#                 # Convert normalized coordinates to pixel values\n",
    "#                 nose_tip_x, nose_tip_y = int(nose_tip.x * w), int(nose_tip.y * h)\n",
    "#                 nose_bridge_x, nose_bridge_y = int(nose_bridge.x * w), int(nose_bridge.y * h)\n",
    "#                 left_ear_x, left_ear_y = int(left_ear.x * w), int(left_ear.y * h)\n",
    "#                 right_ear_x, right_ear_y = int(right_ear.x * w), int(right_ear.y * h)\n",
    "                \n",
    "#                 left_ear_distance= distance(nose_tip_x,nose_tip_y,left_ear_x,left_ear_y)\n",
    "#                 right_ear_distance= distance(nose_tip_x,nose_tip_y,right_ear_x,right_ear_y)\n",
    "#                 # Draw landmarks\n",
    "#                 cv2.circle(image, (nose_tip_x, nose_tip_y), 5, (0, 0, 255), -1)\n",
    "#                 cv2.circle(image, (nose_bridge_x, nose_bridge_y), 5, (0, 255, 0), -1)\n",
    "#                 cv2.circle(image, (left_ear_x, left_ear_y), 5, (255, 0, 0), -1)\n",
    "#                 cv2.circle(image, (right_ear_x, right_ear_y), 5, (255, 0, 0), -1)\n",
    "                \n",
    "#                 # # Condition to check if the ear touches the V-shape of the nose\n",
    "#                 # if abs(left_ear_x - nose_tip_x) < 10 and abs(left_ear_y - nose_tip_y) < 10:\n",
    "#                 #     cv2.putText(image, \"Error: Left ear touching nose V-line\", (50, 50), \n",
    "#                 #                 cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "#                 # if abs(right_ear_x - nose_tip_x) < 10 and abs(right_ear_y - nose_tip_y) < 10:\n",
    "#                 #     cv2.putText(image, \"Error: Right ear touching nose V-line\", (50, 100), \n",
    "#                 #                 cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "#                 if left_ear_distance<20 and right_ear_distance<20:\n",
    "#                     cv2.putText(image, \"Error: Both ears touching nose V-line\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                \n",
    "#                 elif left_ear_distance<20:\n",
    "#                     cv2.putText(image, \"Error: Left ear touching nose V-line\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0 , 255), 2)\n",
    "                \n",
    "#                 elif right_ear_distance<20:\n",
    "#                     cv2.putText(image, \"Error: Right ear touching nose V-line\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0 , 255), 2)\n",
    "\n",
    "#         cv2.imshow('Pose Detection', image)\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     detect_pose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742451612.197774   13003 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742451612.204804   13293 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: Mesa Intel(R) UHD Graphics 620 (KBL GT2)\n",
      "W0000 00:00:1742451612.388076   13287 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742451612.473998   13288 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "\n",
    "def detect_pose():\n",
    "    mp_pose = mp.solutions.pose\n",
    "    mp_draw = mp.solutions.drawing_utils\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    \n",
    "    cap = cv2.VideoCapture(1)\n",
    "    \n",
    "    def distance(x1, y1, x2, y2):\n",
    "        return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "    \n",
    "    def point_to_line_distance(px, py, x1, y1, x2, y2):\n",
    "        num = abs((x2 - x1) * (y1 - py) - (x1 - px) * (y2 - y1))\n",
    "        den = distance(x1, y1, x2, y2)\n",
    "        return num / den if den != 0 else float('inf')\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            h, w, _ = image.shape\n",
    "            \n",
    "            # Draw pose landmarks\n",
    "            mp_draw.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                   mp_draw.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                                   mp_draw.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2))\n",
    "            \n",
    "            # Get landmark positions\n",
    "            nose = results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE]\n",
    "            left_ear = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EAR]\n",
    "            right_ear = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EAR]\n",
    "            left_eye_inner = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EYE_INNER]\n",
    "            right_eye_inner = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EYE_INNER]\n",
    "\n",
    "            # Convert normalized coordinates to pixel values\n",
    "            nose_x, nose_y = int(nose.x * w), int(nose.y * h)\n",
    "            left_ear_x, left_ear_y = int(left_ear.x * w), int(left_ear.y * h)\n",
    "            right_ear_x, right_ear_y = int(right_ear.x * w), int(right_ear.y * h)\n",
    "            left_eye_x, left_eye_y = int(left_eye_inner.x * w), int(left_eye_inner.y * h)\n",
    "            right_eye_x, right_eye_y = int(right_eye_inner.x * w), int(right_eye_inner.y * h)\n",
    "            \n",
    "            # Compute distances from ears to the line between eyes and nose\n",
    "            left_ear_dist = point_to_line_distance(left_ear_x, left_ear_y, left_eye_x, left_eye_y, nose_x, nose_y)\n",
    "            right_ear_dist = point_to_line_distance(right_ear_x, right_ear_y, right_eye_x, right_eye_y, nose_x, nose_y)\n",
    "            \n",
    "            # Draw landmarks\n",
    "            cv2.circle(image, (nose_x, nose_y), 5, (0, 0, 255), -1)\n",
    "            cv2.circle(image, (left_ear_x, left_ear_y), 5, (255, 0, 0), -1)\n",
    "            cv2.circle(image, (right_ear_x, right_ear_y), 5, (255, 0, 0), -1)\n",
    "            cv2.line(image, (left_eye_x, left_eye_y), (nose_x, nose_y), (0, 255, 0), 2)\n",
    "            cv2.line(image, (right_eye_x, right_eye_y), (nose_x, nose_y), (0, 255, 0), 2)\n",
    "            \n",
    "            # Condition to check if the ear touches the eye-nose line\n",
    "            threshold = 10  # Adjust threshold as needed\n",
    "            if left_ear_dist < threshold:\n",
    "                cv2.putText(image, \"Flag: Left ear touching eye-nose line\", (50, 50), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "            if right_ear_dist < threshold:\n",
    "                cv2.putText(image, \"Flag: Right ear touching eye-nose line\", (50, 100), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "        cv2.imshow('Pose Detection', image)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detect_pose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742454082.801995   13003 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742454082.804172   14997 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: Mesa Intel(R) UHD Graphics 620 (KBL GT2)\n",
      "W0000 00:00:1742454082.945424   14987 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742454083.031940   14990 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "\n",
    "def detect_pose():\n",
    "    mp_pose = mp.solutions.pose\n",
    "    mp_draw = mp.solutions.drawing_utils\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    \n",
    "    cap = cv2.VideoCapture(1)\n",
    "    \n",
    "    def distance(x1, y1, x2, y2):\n",
    "        return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "    \n",
    "    def point_to_line_distance(px, py, x1, y1, x2, y2):\n",
    "        num = abs((x2 - x1) * (y1 - py) - (x1 - px) * (y2 - y1))\n",
    "        den = distance(x1, y1, x2, y2)\n",
    "        return num / den if den != 0 else float('inf')\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            h, w, _ = image.shape\n",
    "            \n",
    "            # Draw pose landmarks\n",
    "            mp_draw.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                   mp_draw.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                                   mp_draw.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2))\n",
    "            \n",
    "            # Get landmark positions\n",
    "            nose = results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE]\n",
    "            left_ear = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EAR]\n",
    "            right_ear = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EAR]\n",
    "            left_eye_inner = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EYE_INNER]\n",
    "            right_eye_inner = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EYE_INNER]\n",
    "\n",
    "            # Convert normalized coordinates to pixel values\n",
    "            nose_x, nose_y = int(nose.x * w), int(nose.y * h)\n",
    "            left_ear_x, left_ear_y = int(left_ear.x * w), int(left_ear.y * h)\n",
    "            right_ear_x, right_ear_y = int(right_ear.x * w), int(right_ear.y * h)\n",
    "            left_eye_x, left_eye_y = int(left_eye_inner.x * w), int(left_eye_inner.y * h)\n",
    "            right_eye_x, right_eye_y = int(right_eye_inner.x * w), int(right_eye_inner.y * h)\n",
    "            \n",
    "            # Compute distances from ears to the line between eyes and nose\n",
    "            left_ear_dist = point_to_line_distance(left_ear_x, left_ear_y, left_eye_x, left_eye_y, nose_x, nose_y)\n",
    "            right_ear_dist = point_to_line_distance(right_ear_x, right_ear_y, right_eye_x, right_eye_y, nose_x, nose_y)\n",
    "            \n",
    "            # Draw landmarks\n",
    "            cv2.circle(image, (nose_x, nose_y), 5, (0, 0, 255), -1)\n",
    "            cv2.circle(image, (left_ear_x, left_ear_y), 5, (255, 0, 0), -1)\n",
    "            cv2.circle(image, (right_ear_x, right_ear_y), 5, (255, 0, 0), -1)\n",
    "            cv2.line(image, (left_eye_x, left_eye_y), (nose_x, nose_y), (0, 255, 0), 2)\n",
    "            cv2.line(image, (right_eye_x, right_eye_y), (nose_x, nose_y), (0, 255, 0), 2)\n",
    "            \n",
    "            # Condition to check if the ear touches the eye-nose line\n",
    "            threshold = 10  # Adjust threshold as needed\n",
    "            if left_ear_dist < threshold:\n",
    "                cv2.putText(image, \"Flag: Left ear touching eye-nose line\", (50, 50), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "            if right_ear_dist < threshold:\n",
    "                cv2.putText(image, \"Flag: Right ear touching eye-nose line\", (50, 100), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "        cv2.imshow('Pose Detection', image)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detect_pose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "\n",
    "# def detect_face_mesh():\n",
    "#     mp_face_mesh = mp.solutions.face_mesh\n",
    "#     mp_drawing = mp.solutions.drawing_utils\n",
    "#     mp_drawing_styles = mp.solutions.drawing_styles\n",
    "    \n",
    "#     face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    \n",
    "#     cap = cv2.VideoCapture(1)\n",
    "    \n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         results = face_mesh.process(image)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "#         if results.multi_face_landmarks:\n",
    "#             for face_landmarks in results.multi_face_landmarks:\n",
    "#                 mp_drawing.draw_landmarks(\n",
    "#                     image, face_landmarks, mp_face_mesh.FACEMESH_TESSELATION,\n",
    "#                     mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1),\n",
    "#                     mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=1, circle_radius=1))\n",
    "\n",
    "#         cv2.imshow('Face Mesh Detection', image)\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     detect_face_mesh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "# import numpy as np\n",
    "\n",
    "# def detect_face_mesh():\n",
    "#     mp_face_mesh = mp.solutions.face_mesh\n",
    "#     mp_drawing = mp.solutions.drawing_utils\n",
    "#     mp_drawing_styles = mp.solutions.drawing_styles\n",
    "    \n",
    "#     face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    \n",
    "#     cap = cv2.VideoCapture(1)\n",
    "    \n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         results = face_mesh.process(image)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "#         if results.multi_face_landmarks:\n",
    "#             for face_landmarks in results.multi_face_landmarks:\n",
    "#                 mp_drawing.draw_landmarks(\n",
    "#                     image, face_landmarks, mp_face_mesh.FACEMESH_TESSELATION,\n",
    "#                     mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1),\n",
    "#                     mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=1, circle_radius=1))\n",
    "                \n",
    "#                 # Get face landmarks as a convex hull\n",
    "#                 h, w, _ = image.shape\n",
    "#                 points = np.array([[int(lm.x * w), int(lm.y * h)] for lm in face_landmarks.landmark], dtype=np.int32)\n",
    "#                 if len(points) > 2:\n",
    "#                     face_hull = cv2.convexHull(points)\n",
    "#                 else:\n",
    "#                     continue\n",
    "                \n",
    "#                 # Convert to grayscale and apply Canny edge detection\n",
    "#                 gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#                 edges = cv2.Canny(gray, 50, 150)\n",
    "                \n",
    "#                 # Find external contours\n",
    "#                 contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#                 for contour in contours:\n",
    "#                     for point in contour:\n",
    "#                         pt = tuple(map(int, point[0]))  # Ensure pt is a tuple of integers\n",
    "#                         if face_hull.shape[0] > 2 and cv2.pointPolygonTest(face_hull, pt, False) >= 0:\n",
    "#                             cv2.putText(image, \"Warning: Object in front of face!\", (50, 50), \n",
    "#                                         cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "#                             break\n",
    "                \n",
    "#                 # Draw detected contours for debugging\n",
    "#                 cv2.drawContours(image, contours, -1, (255, 0, 0), 2)\n",
    "        \n",
    "#         cv2.imshow('Face Mesh Detection', image)\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     detect_face_mesh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "# import numpy as np\n",
    "\n",
    "# def detect_face_mesh():\n",
    "#     mp_face_mesh = mp.solutions.face_mesh\n",
    "#     mp_drawing = mp.solutions.drawing_utils\n",
    "#     mp_drawing_styles = mp.solutions.drawing_styles\n",
    "    \n",
    "#     face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    \n",
    "#     cap = cv2.VideoCapture(1)\n",
    "    \n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         results = face_mesh.process(image)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "#         if results.multi_face_landmarks:\n",
    "#             for face_landmarks in results.multi_face_landmarks:\n",
    "#                 mp_drawing.draw_landmarks(\n",
    "#                     image, face_landmarks, mp_face_mesh.FACEMESH_TESSELATION,\n",
    "#                     mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1),\n",
    "#                     mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=1, circle_radius=1))\n",
    "                \n",
    "#                 # Get face landmarks as a convex hull\n",
    "#                 h, w, _ = image.shape\n",
    "#                 points = np.array([[int(lm.x * w), int(lm.y * h)] for lm in face_landmarks.landmark], dtype=np.int32)\n",
    "#                 if len(points) > 2:\n",
    "#                     face_hull = cv2.convexHull(points)\n",
    "#                 else:\n",
    "#                     continue\n",
    "                \n",
    "#                 # Convert to grayscale and apply Canny edge detection\n",
    "#                 gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#                 edges = cv2.Canny(gray, 50, 150)\n",
    "                \n",
    "#                 # Find external contours\n",
    "#                 contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#                 object_detected = False  # Flag to track if an external object is inside face area\n",
    "                \n",
    "#                 for contour in contours:\n",
    "#                     if cv2.contourArea(contour) < 500:  # Ignore small objects\n",
    "#                         continue\n",
    "                    \n",
    "#                     for point in contour:\n",
    "#                         pt = tuple(map(int, point[0]))  # Ensure pt is a tuple of integers\n",
    "#                         if face_hull.shape[0] > 2 and cv2.pointPolygonTest(face_hull, pt, False) > 0:\n",
    "#                             object_detected = True\n",
    "#                             break\n",
    "#                     if object_detected:\n",
    "#                         break\n",
    "                \n",
    "#                 if object_detected:\n",
    "#                     cv2.putText(image, \"Warning: Object in front of face!\", (50, 50), \n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                \n",
    "#                 # Draw detected contours for debugging\n",
    "#                 cv2.drawContours(image, contours, -1, (255, 0, 0), 2)\n",
    "        \n",
    "#         cv2.imshow('Face Mesh Detection', image)\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     detect_face_mesh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742457487.722490   16252 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742457487.723662   17332 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: Mesa Intel(R) UHD Graphics 620 (KBL GT2)\n",
      "I0000 00:00:1742457487.733075   16252 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742457487.734281   17343 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: Mesa Intel(R) UHD Graphics 620 (KBL GT2)\n",
      "W0000 00:00:1742457487.773432   17333 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742457487.797454   17338 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742457487.835350   17324 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742457487.882250   17322 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "\n",
    "def detect_pose():\n",
    "    mp_pose = mp.solutions.pose\n",
    "    mp_hand = mp.solutions.hands\n",
    "    mp_draw = mp.solutions.drawing_utils\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    hands = mp_hand.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "    cap = cv2.VideoCapture(1)\n",
    "\n",
    "    def distance(x1, y1, x2, y2):\n",
    "        return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "\n",
    "    def point_to_line_distance(px, py, x1, y1, x2, y2):\n",
    "        num = abs((x2 - x1) * (y1 - py) - (x1 - px) * (y2 - y1))\n",
    "        den = distance(x1, y1, x2, y2)\n",
    "        return num / den if den != 0 else float('inf')\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image)\n",
    "        hand_result = hands.process(image)\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            h, w, _ = image.shape\n",
    "\n",
    "            # Draw pose landmarks\n",
    "            mp_draw.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                   mp_draw.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                                   mp_draw.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2))\n",
    "\n",
    "            # Get landmark positions\n",
    "            nose = results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE]\n",
    "            left_ear = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EAR]\n",
    "            right_ear = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EAR]\n",
    "            left_eye_inner = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_EYE_INNER]\n",
    "            right_eye_inner = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_EYE_INNER]\n",
    "            right_elbow= results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_ELBOW]\n",
    "            left_elbow = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_ELBOW]\n",
    "\n",
    "            # Convert normalized coordinates to pixel values\n",
    "            nose_x, nose_y = int(nose.x * w), int(nose.y * h)\n",
    "            left_ear_x, left_ear_y = int(left_ear.x * w), int(left_ear.y * h)\n",
    "            right_ear_x, right_ear_y = int(right_ear.x * w), int(right_ear.y * h)\n",
    "            left_eye_x, left_eye_y = int(left_eye_inner.x * w), int(left_eye_inner.y * h)\n",
    "            right_eye_x, right_eye_y = int(right_eye_inner.x * w), int(right_eye_inner.y * h)\n",
    "            right_elbow_x, right_elbow_y = int(right_elbow.x * w), int (right_elbow.y * h)\n",
    "            left_elbow_x, left_elbow_y = int(left_elbow.x * w), int (left_elbow.y * h)\n",
    "\n",
    "\n",
    "            # Compute distances from ears to the line between eyes and nose\n",
    "            left_ear_dist = point_to_line_distance(left_ear_x, left_ear_y, left_eye_x, left_eye_y, nose_x, nose_y)\n",
    "            right_ear_dist = point_to_line_distance(right_ear_x, right_ear_y, right_eye_x, right_eye_y, nose_x, nose_y)\n",
    "\n",
    "            # Draw landmarks\n",
    "            cv2.circle(image, (nose_x, nose_y), 5, (0, 0, 255), -1)\n",
    "            cv2.circle(image, (left_ear_x, left_ear_y), 5, (255, 0, 0), -1)\n",
    "            cv2.circle(image, (right_ear_x, right_ear_y), 5, (255, 0, 0), -1)\n",
    "            cv2.line(image, (left_eye_x, left_eye_y), (nose_x, nose_y), (0, 255, 0), 2)\n",
    "            cv2.line(image, (right_eye_x, right_eye_y), (nose_x, nose_y), (0, 255, 0), 2)\n",
    "\n",
    "            # Condition to check if the ear touches the eye-nose line\n",
    "            threshold = 10  # Adjust threshold as needed\n",
    "            if left_ear_dist < threshold:\n",
    "                cv2.putText(image, \"Flag: Left ear touching eye-nose line\", (50, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "            if right_ear_dist < threshold:\n",
    "                cv2.putText(image, \"Flag: Right ear touching eye-nose line\", (50, 100),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "            \n",
    "            if right_elbow.visibility > 0.5 or left_elbow.visibility > 0.5:\n",
    "                cv2.putText(image, \"Flag: Elbow detected\", (50, 150), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        # Draw hand landmarks if detected\n",
    "        if hand_result.multi_hand_landmarks:\n",
    "            for hand_landmarks in hand_result.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(image, hand_landmarks, mp_hand.HAND_CONNECTIONS,\n",
    "                                       mp_draw.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "                                       mp_draw.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2))\n",
    "                \n",
    "                if hand_landmarks:\n",
    "                    cv2.putText(image, \"Flag: Hand detected\", (50, 100),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "\n",
    "\n",
    "        cv2.imshow('Pose Detection', image)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detect_pose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "# import numpy as np\n",
    "\n",
    "# # Initialize Mediapipe Face Mesh\n",
    "# mp_face_mesh = mp.solutions.face_mesh\n",
    "# mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# # Face mesh landmarks for eyes\n",
    "# LEFT_EYE_LANDMARKS = [33, 160, 158, 133, 153, 144]\n",
    "# RIGHT_EYE_LANDMARKS = [263, 387, 385, 362, 373, 380]\n",
    "\n",
    "# # Initialize Face Mesh\n",
    "# face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# # Capture Video\n",
    "# cap = cv2.VideoCapture(1)\n",
    "\n",
    "# # Store previous eye positions\n",
    "# prev_left_eye_center = None\n",
    "# prev_right_eye_center = None\n",
    "\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Convert frame to RGB for MediaPipe\n",
    "#     image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     results = face_mesh.process(image)\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#     h, w, _ = image.shape  # Get frame dimensions\n",
    "\n",
    "#     if results.multi_face_landmarks:\n",
    "#         for face_landmarks in results.multi_face_landmarks:\n",
    "#             # Get eye positions\n",
    "#             left_eye_points = np.array([(int(face_landmarks.landmark[i].x * w), int(face_landmarks.landmark[i].y * h)) for i in LEFT_EYE_LANDMARKS])\n",
    "#             right_eye_points = np.array([(int(face_landmarks.landmark[i].x * w), int(face_landmarks.landmark[i].y * h)) for i in RIGHT_EYE_LANDMARKS])\n",
    "\n",
    "#             # Compute eye centers\n",
    "#             left_eye_center = np.mean(left_eye_points, axis=0).astype(int)\n",
    "#             right_eye_center = np.mean(right_eye_points, axis=0).astype(int)\n",
    "\n",
    "#             # Draw eye landmarks\n",
    "#             for point in left_eye_points:\n",
    "#                 cv2.circle(image, tuple(point), 2, (0, 255, 0), -1)\n",
    "#             for point in right_eye_points:\n",
    "#                 cv2.circle(image, tuple(point), 2, (0, 255, 0), -1)\n",
    "                \n",
    "\n",
    "#             # Draw center points\n",
    "#             cv2.circle(image, tuple(left_eye_center), 3, (0, 0, 255), -1)\n",
    "#             cv2.circle(image, tuple(right_eye_center), 3, (0, 0, 255), -1)\n",
    "\n",
    "#             # Detect movement\n",
    "#             if prev_left_eye_center is not None and prev_right_eye_center is not None:\n",
    "#                 left_eye_movement = np.linalg.norm(left_eye_center - prev_left_eye_center)\n",
    "#                 right_eye_movement = np.linalg.norm(right_eye_center - prev_right_eye_center)\n",
    "\n",
    "#                 if left_eye_movement > 3 or right_eye_movement > 3:  # Threshold for movement detection\n",
    "#                     cv2.putText(image, \"Eye Movement Detected\", (50, 50),\n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "#             # Update previous positions\n",
    "#             prev_left_eye_center = left_eye_center\n",
    "#             prev_right_eye_center = right_eye_center\n",
    "\n",
    "#     cv2.imshow('Eye Movement Detection', image)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742457689.558898   16252 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742457689.560338   17448 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: Mesa Intel(R) UHD Graphics 620 (KBL GT2)\n",
      "W0000 00:00:1742457689.567118   17441 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742457689.582688   17438 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Mediapipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Eye landmark indices from MediaPipe Face Mesh\n",
    "LEFT_EYE_LANDMARKS = [33, 160, 158, 133, 153, 144, 145, 159, 23]\n",
    "RIGHT_EYE_LANDMARKS = [263, 387, 385, 362, 373, 380, 374, 386, 253]\n",
    "\n",
    "# Initialize Face Mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Capture Video\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Store previous eye positions\n",
    "prev_left_eye = None\n",
    "prev_right_eye = None\n",
    "movement_threshold = 5  # Minimum pixel movement to trigger warning\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frame to RGB for MediaPipe\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    h, w, _ = image.shape  # Get frame dimensions\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # Get left and right eye landmark positions\n",
    "            left_eye_points = [(int(face_landmarks.landmark[i].x * w), int(face_landmarks.landmark[i].y * h)) for i in LEFT_EYE_LANDMARKS]\n",
    "            right_eye_points = [(int(face_landmarks.landmark[i].x * w), int(face_landmarks.landmark[i].y * h)) for i in RIGHT_EYE_LANDMARKS]\n",
    "\n",
    "            # Calculate the center of the left and right eye\n",
    "            left_eye_center = np.mean(left_eye_points, axis=0).astype(int)\n",
    "            right_eye_center = np.mean(right_eye_points, axis=0).astype(int)\n",
    "\n",
    "            # Draw left and right eye centers\n",
    "            cv2.circle(image, tuple(left_eye_center), 3, (0, 255, 0), -1)\n",
    "            cv2.circle(image, tuple(right_eye_center), 3, (0, 255, 0), -1)\n",
    "\n",
    "            # Detect eye movement\n",
    "            if prev_left_eye is not None and prev_right_eye is not None:\n",
    "                left_movement = np.linalg.norm(left_eye_center - prev_left_eye)\n",
    "                right_movement = np.linalg.norm(right_eye_center - prev_right_eye)\n",
    "\n",
    "                if left_movement > movement_threshold or right_movement > movement_threshold:\n",
    "                    cv2.putText(image, \"WARNING: Eye Movement Detected!\", (50, 50), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "\n",
    "            # Update previous eye positions\n",
    "            prev_left_eye = left_eye_center\n",
    "            prev_right_eye = right_eye_center\n",
    "    else:\n",
    "        cv2.putText(image, \"No face detected\", (50, 50), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Eye Movement Detection', image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
